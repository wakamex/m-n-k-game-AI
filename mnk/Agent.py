import random
import numpy as np
import time

from mnk.constants import EMPTY, NOONE, MAX_TIME

class Agent:
    def __init__(self, player_number, board_size, winning_size, scoring_array, circle_of_two, name="Agent", depth=3): # Added depth parameter with default
        """Initialize the agent with game parameters"""
        self.player_number = player_number
        self.board_size = tuple(board_size) if isinstance(board_size, list) else board_size if isinstance(board_size, tuple) else (board_size, board_size)
        self.winning_size = winning_size
        self.scoring_array = scoring_array # Still here, for future heuristic
        self.name = name
        self.circle_of_two = circle_of_two
        self.search_depth = depth # Store depth as an instance variable

        # Initialize memory
        self.memory = {
            "board_state": np.full(self.board_size[0] * self.board_size[1], EMPTY, dtype=np.int32), # Use np.full for consistency
            "last_move": None,
            "counts": None,
            "last_move_played": None
        }
        self.game = None  # Will be set when added to a game

        # Store directions as flattened array like in Mojo
        self.directions = np.array([
            1, 0,   # right
            0, 1,   # down
            1, 1,   # down-right
            1, -1,  # up-right
        ], dtype=np.int32)

        # Debug counters (optional, but can be useful)
        self.states_evaluated = 0
        self.max_depth_reached_in_last_move = 0 # Renamed for clarity
        self.last_move_start_time = None # Renamed for clarity

    def set_game(self, game):
        """Set the game instance for this agent"""
        self.game = game

    def forget(self):
        """Reset agent state for a new game"""
        self.memory = {
            "board_state": np.full(self.board_size[0] * self.board_size[1], EMPTY, dtype=np.int32),
            "last_move": None,
            "counts": None,
            "last_move_played": None
        }

    def get_next_move(self):
        """Get the next move for this agent"""
        # Reset debug counters
        self.states_evaluated = 0
        self.max_depth_reached_in_last_move = 0 # Reset specific counter
        self.last_move_start_time = time.time() # Reset specific timer

        print(f"\n{self.name} (Player {self.player_number}, Depth {self.search_depth}) thinking...")

        best_move_index = -1 # Store index of the move
        # Best value for P0 (maximizer) is -inf, for P1 (minimizer) is +inf
        best_value = float("-inf") if self.player_number == 0 else float("inf")

        # Current game state for the agent to consider
        current_board_state_for_minimax = self.game.board.copy()
        
        # Generate possible moves from the current game board state
        # The generate_next_moves method needs the board state and whose turn it is.
        # It should generate moves for the current agent (self.player_number).
        
        # Create an initial 'state' dict for generate_next_moves, similar to how minimax receives it.
        # The 'last_move' in this initial state isn't strictly necessary if generate_next_moves only uses board_state.
        initial_state_for_move_gen = {
            "board_state": current_board_state_for_minimax,
            "last_move": self.memory["last_move"] # or None, depends on generate_next_moves
        }
        
        next_possible_moves_states = self.generate_next_moves(initial_state_for_move_gen, self.player_number)

        if not next_possible_moves_states:
            # This case should ideally be handled by is_game_over (e.g., a draw if no moves left)
            # or if generate_next_moves can return empty when it's not a terminal draw state (e.g. zugzwang in other games)
            # For m,n,k this usually means board is full.
            empty_spaces = [i for i, x in enumerate(current_board_state_for_minimax) if x == EMPTY]
            if empty_spaces: # Should not happen if game isn't over.
                 print(f"Warning: No moves generated by generate_next_moves but board is not full for {self.name}.")
                 return empty_spaces[0] # Fallback: pick first available
            else: # Board is full, likely a draw if no winner yet
                 print(f"Warning: No moves generated by generate_next_moves, board is full for {self.name}.")
                 # This scenario should ideally be caught by game.is_game_over() before calling get_next_move.
                 # If it reaches here, it's an issue or an unhandled terminal state.
                 # Raising an error might be appropriate if the game isn't already flagged as over.
                 # For now, let's assume the game loop handles this. If a move is forced, what to return?
                 # This implies a logic error elsewhere if called when no moves are possible and game not over.
                 # For robustness, if somehow called in such a state:
                 raise ValueError(f"{self.name} called for move, but no moves possible and game not flagged as over.")


        for move_state in next_possible_moves_states:
            # Check if we've exceeded time limit
            if time.time() - self.last_move_start_time > MAX_TIME:
                print(f"{self.name} timed out during move selection.")
                if best_move_index == -1: # If no good move found yet
                    # Fallback: return the first move from the generated list
                    # (or a random one from the list if preferred)
                    print(f"Timeout fallback: selecting first generated move: {next_possible_moves_states[0]['last_move']}")
                    return next_possible_moves_states[0]["last_move"]
                break # Exit loop, use best_move found so far

            # The `move_state` is the state *after* the current agent makes a hypothetical move.
            # So, the next call to minimax is for the *opponent* to play from this `move_state`.
            # Therefore, `maximizing_player` for the next call is `not (self.player_number == 0)`.
            # The `depth` for the initial call to minimax from get_next_move should be `self.search_depth -1`
            # because `self.search_depth` includes the current move being considered.
            # Or, more clearly, minimax explores `self.search_depth` *plies down from the opponent's turn*.
            # If self.search_depth = 1, minimax is called with depth 0 (evaluate current board after move).
            
            value = self.minimax(move_state, self.search_depth -1, float("-inf"), float("inf"), self.player_number == 1) # True if P1 (agent) just moved, so P0 (opponent) is maximizing
            
            # Debug output for critical positions
            if self.player_number == 1 and self.search_depth == 1:
                print(f"  Move {move_state['last_move']}: value = {value:.2f}")

            if self.player_number == 0:  # Agent is P0 (Maximizing player)
                if value > best_value:
                    best_value = value
                    best_move_index = move_state["last_move"]
            else:  # Agent is P1 (Minimizing player)
                if value < best_value:
                    best_value = value
                    best_move_index = move_state["last_move"]
        
        if best_move_index == -1 :
            # This could happen if all moves timed out before one was fully evaluated,
            # or if next_possible_moves_states was empty initially (handled above).
            # Or if all evaluated moves somehow resulted in values that didn't update best_value (e.g. all -inf for maximizer)
            print(f"Warning: {self.name} did not select a best move. Fallback to first generated move.")
            if next_possible_moves_states: # Ensure list is not empty
                 return next_possible_moves_states[0]["last_move"]
            else: # Should have been caught earlier, but as a safeguard
                 raise ValueError(f"{self.name} has no moves to select from.")

        print(f"{self.name} selected move: {best_move_index} with value: {best_value:.2f}. States evaluated: {self.states_evaluated}. Max depth reached: {self.max_depth_reached_in_last_move}")
        return best_move_index

    def new_move_played(self, board_state):
        """Update our memory with the new board state"""
        last_move = None
        # Compare current self.memory["board_state"] with the new board_state
        # This assumes self.memory["board_state"] is the state *before* the latest move.
        current_length = len(self.memory["board_state"])
        new_length = len(board_state)

        # Ensure consistent board representation (numpy array) for comparison
        if not isinstance(self.memory["board_state"], np.ndarray):
            self.memory["board_state"] = np.array(self.memory["board_state"], dtype=np.int32)
        if not isinstance(board_state, np.ndarray):
            board_state_np = np.array(board_state, dtype=np.int32)
        else:
            board_state_np = board_state

        if current_length != new_length:
            # This might happen on first move if memory not initialized to same size, or error
            print(f"Warning: Board length mismatch in new_move_played for {self.name}. Mem: {current_length}, New: {new_length}")
            # Fallback: try to find the first differing element if lengths are same, else cannot determine
            if current_length == new_length: # Should not happen if this far
                 diff_indices = np.where(self.memory["board_state"] != board_state_np)[0]
                 if len(diff_indices) == 1:
                     last_move = diff_indices[0]
        else:
            diff_indices = np.where(self.memory["board_state"] != board_state_np)[0]
            if len(diff_indices) == 1:
                last_move = diff_indices[0]
            elif len(diff_indices) > 1:
                # This could happen if multiple moves are made between agent updates, or first move.
                # If it's the first move, self.memory["board_state"] is all EMPTY.
                # We assume only one piece changed.
                # A better way for first move: if self.memory["last_move"] is None, find the only piece.
                if np.all(self.memory["board_state"] == EMPTY): # Likely the first move of the game
                    non_empty_indices = np.where(board_state_np != EMPTY)[0]
                    if len(non_empty_indices) == 1:
                        last_move = non_empty_indices[0]
                else:
                    print(f"Warning: Multiple differences found in board state for {self.name}, cannot reliably determine last move. Diffs: {diff_indices}")


        self.memory["board_state"] = board_state_np.copy()
        self.memory["last_move"] = last_move # Store the index of the move
        self.memory["counts"] = None  # Reset counts when board changes, will be re-calculated by evaluate/minimax

        if last_move is not None:
            self.memory["last_move_played"] = np.array([last_move % self.board_size[0], last_move // self.board_size[0]], dtype=np.int32)
        else:
            # If last_move couldn't be determined (e.g., on first call with an empty board to initialize)
            self.memory["last_move_played"] = None


    # count_sequences, is_move_too_far_from_action, generate_next_moves, is_game_over, evaluate
    # remain the same for now.
    # ... (rest of the Agent class methods: count_sequences, is_move_too_far_from_action, etc.) ...

    def count_sequences(self, board_state_param): # Renamed param to avoid conflict
        # Ensure board_state is a numpy array
        if not isinstance(board_state_param, np.ndarray):
            board_state = np.array(board_state_param, dtype=np.int32)
        else:
            board_state = board_state_param

        counts = np.zeros((2, self.winning_size + 1), dtype=np.int32)
        empty_count = np.sum(board_state == EMPTY)
        counts[0][0] = empty_count
        counts[1][0] = empty_count

        for player in range(2): # 0 and 1
            # Horizontal
            for r in range(self.board_size[1]):
                for c in range(self.board_size[0] - self.winning_size + 1):
                    segment_indices = [r * self.board_size[0] + c + i for i in range(self.winning_size)]
                    current_player_pieces = 0
                    opponent_pieces = 0
                    for idx in segment_indices:
                        if board_state[idx] == player:
                            current_player_pieces += 1
                        elif board_state[idx] != EMPTY:
                            opponent_pieces += 1
                    if opponent_pieces == 0 and current_player_pieces > 0:
                        counts[player][current_player_pieces] += 1
            # Vertical
            for c in range(self.board_size[0]):
                for r in range(self.board_size[1] - self.winning_size + 1):
                    segment_indices = [(r + i) * self.board_size[0] + c for i in range(self.winning_size)]
                    current_player_pieces = 0
                    opponent_pieces = 0
                    for idx in segment_indices:
                        if board_state[idx] == player:
                            current_player_pieces += 1
                        elif board_state[idx] != EMPTY:
                            opponent_pieces += 1
                    if opponent_pieces == 0 and current_player_pieces > 0:
                        counts[player][current_player_pieces] += 1
            # Diagonal down-right
            for r in range(self.board_size[1] - self.winning_size + 1):
                for c in range(self.board_size[0] - self.winning_size + 1):
                    segment_indices = [(r + i) * self.board_size[0] + (c + i) for i in range(self.winning_size)]
                    current_player_pieces = 0
                    opponent_pieces = 0
                    for idx in segment_indices:
                        if board_state[idx] == player:
                            current_player_pieces += 1
                        elif board_state[idx] != EMPTY:
                            opponent_pieces += 1
                    if opponent_pieces == 0 and current_player_pieces > 0:
                        counts[player][current_player_pieces] += 1
            # Diagonal up-right (actually TR to BL)
            for r in range(self.winning_size - 1, self.board_size[1]):
                for c in range(self.board_size[0] - self.winning_size + 1):
                    segment_indices = [(r - i) * self.board_size[0] + (c + i) for i in range(self.winning_size)]
                    current_player_pieces = 0
                    opponent_pieces = 0
                    for idx in segment_indices:
                        if board_state[idx] == player:
                            current_player_pieces += 1
                        elif board_state[idx] != EMPTY:
                            opponent_pieces += 1
                    if opponent_pieces == 0 and current_player_pieces > 0:
                        counts[player][current_player_pieces] += 1
        return counts

    def is_move_too_far_from_action(self, board, move_tuple, circle_of_two_config):
        # Ensure board is numpy array
        if not isinstance(board, np.ndarray):
            board_np = np.array(board, dtype=np.int32)
        else:
            board_np = board

        x, y = move_tuple
        # If board is empty, any move is fine (not too far)
        if np.all(board_np == EMPTY):
            return False

        for dx, dy in circle_of_two_config: # Use the passed config
            new_x, new_y = x + dx, y + dy
            if 0 <= new_x < self.board_size[0] and 0 <= new_y < self.board_size[1]:
                if board_np[new_y * self.board_size[0] + new_x] != EMPTY:
                    return False # Found a nearby piece, so move is not too far
        return True # No nearby pieces found

    def generate_next_moves(self, current_state_dict, current_player_to_move):
        next_moves_states = []
        board = current_state_dict["board_state"]
        # Ensure board is numpy array
        if not isinstance(board, np.ndarray):
            board_np = np.array(board, dtype=np.int32)
        else:
            board_np = board

        # Simple validation (optional, but good for robustness)
        # p0_count_on_board = np.sum(board_np == 0)
        # p1_count_on_board = np.sum(board_np == 1)
        # if current_player_to_move == 0 and p0_count_on_board > p1_count_on_board: return []
        # if current_player_to_move == 1 and p1_count_on_board >= p0_count_on_board: return [] # P1 cannot have more or equal if P0 played last (unless P0 starts)

        any_pieces_on_board = np.any(board_np != EMPTY)

        for i in range(len(board_np)):
            if board_np[i] == EMPTY:
                move_coord = (i % self.board_size[0], i // self.board_size[0])
                # Only add move if it's not too far from action, or if board is empty
                if not any_pieces_on_board or not self.is_move_too_far_from_action(board_np, move_coord, self.circle_of_two):
                    new_board_state_after_move = board_np.copy()
                    new_board_state_after_move[i] = current_player_to_move
                    next_moves_states.append({
                        "board_state": new_board_state_after_move,
                        "last_move": i # The move just made to reach this state
                    })
        return next_moves_states

    def is_game_over(self, state_dict):
        """Check if the current state is a terminal state. state_dict contains 'board_state'."""
        board = state_dict["board_state"]
        if not isinstance(board, np.ndarray): # Ensure numpy array
            board = np.array(board, dtype=np.int32)

        moves_played = np.sum(board != EMPTY)
        if moves_played < self.winning_size * 2 - 1 and moves_played < self.winning_size : # Optimization
             # if P1 plays 3rd move of game, total moves = 3. winning_size = 3. 3 < 3*2-1 (5) is true.
             # if P1 plays 2nd move of game, total moves = 2. winning_size = 3. 2 < 3 is true.
             # This should be min_moves_to_win = self.winning_size
            if moves_played < self.winning_size:
                return False, NOONE # Not NOONE (-1), but an indicator like a different value or tuple

        for r in range(self.board_size[1]):
            for c in range(self.board_size[0]):
                player = board[r * self.board_size[0] + c]
                if player == EMPTY:
                    continue

                # Horizontal
                if c <= self.board_size[0] - self.winning_size:
                    if all(board[r * self.board_size[0] + c + i] == player for i in range(self.winning_size)):
                        return True, player
                # Vertical
                if r <= self.board_size[1] - self.winning_size:
                    if all(board[(r + i) * self.board_size[0] + c] == player for i in range(self.winning_size)):
                        return True, player
                # Diagonal Down-Right
                if r <= self.board_size[1] - self.winning_size and c <= self.board_size[0] - self.winning_size:
                    if all(board[(r + i) * self.board_size[0] + (c + i)] == player for i in range(self.winning_size)):
                        return True, player
                # Diagonal Up-Right (actually TR to BL)
                if r >= self.winning_size - 1 and c <= self.board_size[0] - self.winning_size:
                    if all(board[(r - i) * self.board_size[0] + (c + i)] == player for i in range(self.winning_size)):
                        return True, player
        
        if moves_played == len(board): # Board is full
            return True, -2 # -2 for Draw

        return False, NOONE # Game not over, no winner yet

    def evaluate(self, state_dict, winner_code): # winner_code from is_game_over
        """Evaluate the state from perspective of maximizing player (player 0)"""
        if winner_code == 0:  # Max player (P0) won
            return 1.0
        elif winner_code == 1:  # Min player (P1) won
            return -1.0
        elif winner_code == -2:  # Draw
            return 0.0
        
        # Game is ongoing - use heuristic
        # Ensure board_state from dict is used
        current_board_for_eval = state_dict["board_state"]
        counts = self.count_sequences(current_board_for_eval)
        
        score = (counts[0][2] - counts[1][2]) * 0.1 # P0's "threats" - P1's "threats"
        return max(min(score, 0.9), -0.9)

    def minimax(self, current_node_state_dict, depth, alpha, beta, opponent_is_maximizing):
        """Minimax implementation with alpha-beta pruning.
        current_node_state_dict: The board state to evaluate or expand.
        depth: Remaining depth to search.
        opponent_is_maximizing: True if the player whose turn it is from current_node_state_dict is the maximizing player.
                                (i.e., if it's P0's turn to move from this state).
        """
        self.states_evaluated += 1
        self.max_depth_reached_in_last_move = max(self.search_depth - depth, self.max_depth_reached_in_last_move)

        # Check for terminal state or depth limit
        game_over, winner = self.is_game_over(current_node_state_dict)
        if game_over:
            return self.evaluate(current_node_state_dict, winner)
        if depth == 0:
            return self.evaluate(current_node_state_dict, NOONE) # NOONE indicates game not over, evaluate heuristically

        # Determine whose turn it is to make a move from current_node_state_dict
        # If opponent_is_maximizing is True, it's P0's turn (the maximizing player).
        # If opponent_is_maximizing is False, it's P1's turn (the minimizing player).
        player_to_move_from_this_state = 0 if opponent_is_maximizing else 1
        
        child_states = self.generate_next_moves(current_node_state_dict, player_to_move_from_this_state)

        if not child_states: # No valid moves from this state, but not flagged as game_over earlier
            # This implies a draw that wasn't caught by board full in is_game_over, or a logic issue.
            # Evaluate current state as if it's a draw or based on heuristic if not strictly full.
            # This path should ideally not be hit often if is_game_over is robust.
            return self.evaluate(current_node_state_dict, -2) # Treat as draw

        if opponent_is_maximizing: # Current player (P0) is maximizing
            max_eval = float("-inf")
            for child_state in child_states:
                eval_score = self.minimax(child_state, depth - 1, alpha, beta, False) # Opponent (P1) will minimize
                max_eval = max(max_eval, eval_score)
                alpha = max(alpha, eval_score)
                if beta <= alpha:
                    break
            return max_eval
        else: # Current player (P1) is minimizing
            min_eval = float("inf")
            for child_state in child_states:
                eval_score = self.minimax(child_state, depth - 1, alpha, beta, True) # Opponent (P0) will maximize
                min_eval = min(min_eval, eval_score)
                beta = min(beta, eval_score)
                if beta <= alpha:
                    break
            return min_eval